{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3980c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import hydra\n",
    "import lightning as L\n",
    "import torch\n",
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from omegaconf import DictConfig\n",
    "from src.data_module import DecathlonDataModule\n",
    "#from src.model import DecathlonModel\n",
    "from src.utils import generate_run_id\n",
    "from monai.networks.nets import SegResNet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5cf1d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from monai.losses import DiceLoss\n",
    "from monai.networks.nets import DynUNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import compute_generalized_dice\n",
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "\n",
    "class DecathlonModel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 3e-4,\n",
    "        use_scheduler: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.use_scheduler = use_scheduler\n",
    "\n",
    "        # Define the model\n",
    "        self._model = DynUNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            kernel_size=[3, 3, 3, 3, 3, 3],\n",
    "            strides=[1, 2, 2, 2, 2, 2],\n",
    "            upsample_kernel_size=[2, 2, 2, 2, 2, 2],\n",
    "            norm_name=Norm.BATCH,\n",
    "            deep_supervision=False,\n",
    "        )\n",
    "\n",
    "        # Define the loss function\n",
    "        self.criterion = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[\"image\"], batch[\"label\"]\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    # Define inference method\n",
    "    def _inference(self, input):\n",
    "        def _compute(input):\n",
    "            return sliding_window_inference(\n",
    "                inputs=input,\n",
    "                roi_size=(128, 128, 128),\n",
    "                sw_batch_size=1,\n",
    "                predictor=self,\n",
    "                overlap=0.5,\n",
    "            )\n",
    "\n",
    "        VAL_AMP = True\n",
    "        if VAL_AMP:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                return _compute(input)\n",
    "        else:\n",
    "            return _compute(input)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch[\"image\"], batch[\"label\"]\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        y_pred = self._inference(x)\n",
    "        dice = compute_generalized_dice(y_pred, y)\n",
    "        dice = dice.mean() if len(dice) > 0 else dice\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_dice\", dice, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=self.learning_rate, weight_decay=0.05\n",
    "        )\n",
    "\n",
    "        configuration = {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"monitor\": \"val_loss\",  # monitor validation loss\n",
    "        }\n",
    "\n",
    "        if self.use_scheduler:\n",
    "            # Add lr scheduler\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "            configuration[\"lr_scheduler\"] = scheduler\n",
    "\n",
    "        return configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9c3672cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # generate unique run id based on current date & time\n",
    "    run_id = generate_run_id()\n",
    "\n",
    "    # Seed everything for reproducibility\n",
    "    L.seed_everything(42, workers=True)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    # Initialize DataModule\n",
    "    dm = DecathlonDataModule(\n",
    "        root_dir='./data',\n",
    "        task=\"Task06_Lung\",\n",
    "        batch_size=1,\n",
    "        num_workers=3,\n",
    "        seed=42\n",
    "    )\n",
    "    dm.setup()\n",
    "\n",
    "    # Init model from datamodule's attributes\n",
    "    model = DecathlonModel(\n",
    "        # num_classes=dm.num_classes,\n",
    "        learning_rate= 3e-1,\n",
    "        use_scheduler= True,\n",
    "    )\n",
    "\n",
    "    # Init logger\n",
    "    logger = TensorBoardLogger(save_dir='logs', name=\"\", version=run_id)\n",
    "    # Init callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=2,\n",
    "        dirpath=join(' artifacts/checkpoints', run_id),\n",
    "        filename=\"{epoch}-{step}-{val_loss:.2f}-{val_dice:.2f}\",\n",
    "    )\n",
    "\n",
    "    # Init LearningRateMonitor\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "    # early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,\n",
    "        verbose=True,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, lr_monitor, early_stopping],\n",
    "    precision=16,  # Enable mixed precision training\n",
    ")\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, dm)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "46d05054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train()\n",
      "Cell \u001b[1;32mIn[62], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize DataModule\u001b[39;00m\n\u001b[0;32m     10\u001b[0m dm \u001b[38;5;241m=\u001b[39m DecathlonDataModule(\n\u001b[0;32m     11\u001b[0m     root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask06_Lung\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m dm\u001b[38;5;241m.\u001b[39msetup()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Init model from datamodule's attributes\u001b[39;00m\n\u001b[0;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m DecathlonModel(\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# num_classes=dm.num_classes,\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3e-1\u001b[39m,\n\u001b[0;32m     23\u001b[0m     use_scheduler\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m )\n",
      "File \u001b[1;32m~\\Downloads\\lung-tumours-segmentation-main\\src\\data_module.py:103\u001b[0m, in \u001b[0;36mDecathlonDataModule.setup\u001b[1;34m(self, stage)\u001b[0m\n\u001b[0;32m     57\u001b[0m val_transform \u001b[38;5;241m=\u001b[39m Compose(\n\u001b[0;32m     58\u001b[0m     [\n\u001b[0;32m     59\u001b[0m         LoadImaged(keys\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     ]\n\u001b[0;32m     79\u001b[0m )\n\u001b[0;32m     80\u001b[0m transform \u001b[38;5;241m=\u001b[39m Compose(\n\u001b[0;32m     81\u001b[0m     [\n\u001b[0;32m     82\u001b[0m         LoadImaged(keys\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m     ]\n\u001b[0;32m    101\u001b[0m )\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data \u001b[38;5;241m=\u001b[39m DecathlonDataset(\n\u001b[0;32m    104\u001b[0m     root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/matij/Downloads/lung-tumours-segmentation-main/data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    105\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask,\n\u001b[0;32m    106\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[0;32m    107\u001b[0m     section\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    108\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed,\n\u001b[0;32m    109\u001b[0m     download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    110\u001b[0m     cache_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_data \u001b[38;5;241m=\u001b[39m DecathlonDataset(\n\u001b[0;32m    114\u001b[0m     root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/matij/Downloads/lung-tumours-segmentation-main/data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    115\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m     cache_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m    121\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\monai\\apps\\datasets.py:316\u001b[0m, in \u001b[0;36mDecathlonDataset.__init__\u001b[1;34m(self, root_dir, task, section, transform, download, seed, val_frac, cache_num, cache_rate, num_workers, progress, copy_cache, as_contiguous, runtime_cache)\u001b[0m\n\u001b[0;32m    314\u001b[0m tarfile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m--> 316\u001b[0m     download_and_extract(\n\u001b[0;32m    317\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresource[task],\n\u001b[0;32m    318\u001b[0m         filepath\u001b[38;5;241m=\u001b[39mtarfile_name,\n\u001b[0;32m    319\u001b[0m         output_dir\u001b[38;5;241m=\u001b[39mroot_dir,\n\u001b[0;32m    320\u001b[0m         hash_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmd5[task],\n\u001b[0;32m    321\u001b[0m         hash_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmd5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    322\u001b[0m         progress\u001b[38;5;241m=\u001b[39mprogress,\n\u001b[0;32m    323\u001b[0m     )\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dataset_dir\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find dataset directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, please use download=True to download it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    328\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\monai\\apps\\utils.py:335\u001b[0m, in \u001b[0;36mdownload_and_extract\u001b[1;34m(url, filepath, output_dir, hash_val, hash_type, file_type, has_base, progress)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m tmp_dir:\n\u001b[0;32m    334\u001b[0m     filename \u001b[38;5;241m=\u001b[39m filepath \u001b[38;5;129;01mor\u001b[39;00m Path(tmp_dir, _basename(url))\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[1;32m--> 335\u001b[0m     download_url(url\u001b[38;5;241m=\u001b[39murl, filepath\u001b[38;5;241m=\u001b[39mfilename, hash_val\u001b[38;5;241m=\u001b[39mhash_val, hash_type\u001b[38;5;241m=\u001b[39mhash_type, progress\u001b[38;5;241m=\u001b[39mprogress)\n\u001b[0;32m    336\u001b[0m     extractall(filepath\u001b[38;5;241m=\u001b[39mfilename, output_dir\u001b[38;5;241m=\u001b[39moutput_dir, file_type\u001b[38;5;241m=\u001b[39mfile_type, has_base\u001b[38;5;241m=\u001b[39mhas_base)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\monai\\apps\\utils.py:199\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, filepath, hash_val, hash_type, progress, **gdown_kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m filepath \u001b[38;5;241m=\u001b[39m Path(filepath)\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filepath\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_hash(filepath, hash_val, hash_type):\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhash_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m check of existing file failed: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhash_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhash_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         )\n\u001b[0;32m    203\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile exists: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, skipped downloading.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\monai\\apps\\utils.py:147\u001b[0m, in \u001b[0;36mcheck_hash\u001b[1;34m(filepath, val, hash_type)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m: f\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m), \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 147\u001b[0m             actual_hash\u001b[38;5;241m.\u001b[39mupdate(chunk)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    149\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException in check_hash: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf0061e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26767646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d099d6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
